{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Multilabel active learning algorithms (query strategies): \n",
    "  1) Binary Minimization (BinMin)\n",
    "     Calculates the uncertainty of each label independently\n",
    "  2) Maximal Loss Reduction with Maximal Confidence (MMC)\n",
    "     Uncertainty based upon the difference between predictions from two\n",
    "     different multilabel classifiers. MMC uses two multilabel classifiers:\n",
    "     i) binary relevance\n",
    "     ii) stacked logistic regression\n",
    "  3) Multilabel Active Learning With Auxiliary Learner (MLALAL)\n",
    "     As with MMC, uncertainty based upon the difference between predictions \n",
    "     from two different multilabel classifiers\n",
    "  4) Random Sampling (used as a baseline for comparision)\n",
    "  TO DO 5) Adaptive Active Learning (multilabel/adaptive_active_learning.py)\n",
    "\n",
    "Query Criteria (i.e. options for evaluating the prediction differences) \n",
    "used for MMC and MLALAL query strategies\n",
    "i) Hamming Loss Reduction (HLR)\n",
    "ii) Soft Hamming Loss Reduction (SHLR)\n",
    "iii) Maximum Margin Reduction (MMR)\n",
    "\n",
    "The Maluuba dataset contains 1367 conversations, which gives us 19,984 \n",
    "examples. Each of these examples can have upto 20 labels\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from word_vec_fn import full_trn_tst, experiments\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences and labels loaded.\n"
     ]
    }
   ],
   "source": [
    "data_in = './data/maluuba/data_frames.json'\n",
    "\n",
    "\n",
    "trn_ds, tst_ds, fully_labeled_trn_ds = full_trn_tst(data_in,\n",
    "                                                    test_size=0.25, \n",
    "                                                    num_labelled = 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before qs1\n",
      "before qs2\n",
      "before qs3\n"
     ]
    }
   ],
   "source": [
    "# After each of the 10 queries the classifier will be retrained\n",
    "quota=10\n",
    "\n",
    "results = experiments(fully_labeled_trn_ds, trn_ds, tst_ds, quota)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_num = np.arange(1, quota + 1)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 3)) # increase plot size\n",
    "plt.plot(query_num, results['MMC'], 'g', label='MMC')\n",
    "plt.plot(query_num, results['Random'], 'k', label='Random')\n",
    "plt.plot(query_num, results['Aux_hlr'][0], 'r', label='AuxiliaryLearner_hlr')\n",
    "plt.plot(query_num, results['Aux_shlr'][0], 'b', label='AuxiliaryLearner_shlr')\n",
    "plt.plot(query_num, results['Aux_mmr'][0], 'c', label='AuxiliaryLearner_mmr')\n",
    "plt.plot(query_num, results['BinMin'][0], 'm', label='BinaryMinimization')\n",
    "\n",
    "plt.title('Experiment Result (Hamming Loss)', fontsize=12)\n",
    "plt.xlabel('Number of Queries', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.legend(fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1) How many queries would it take to get 90% of the (hamming) loss, we would get if we had access to all labelled samples?\n",
    "# 2) Add hyperparameters (taken as arguments at the command line) for removal of stop words\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py2]",
   "language": "python",
   "name": "conda-env-py2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
