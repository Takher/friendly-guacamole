{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "\n",
    "\n",
    "def load_glove_model(gloveFile, word_count=100000):\n",
    "    \"\"\"\n",
    "    Loads a selection of the most common glove vectors into a dictionary.\n",
    "    :param gloveFile: textfile\n",
    "        Contains glove vectors. Each line contains a word string followed by a\n",
    "        'n_features'-dimensional vector to describe the word. Where n_features\n",
    "        is the number of features.\n",
    "    :param word_count: int, default: 100000\n",
    "        Number of words to load from the gloveFile\n",
    "    :return: dictionary\n",
    "        {'word': vector}\n",
    "        word = string of the word we wish to load\n",
    "        vector = 'n_features'-d vectors to describe the word\n",
    "    \"\"\"\n",
    "    path = './data/gloveFile_done_%d.npy' % (word_count)\n",
    "\n",
    "    # Saves time by loading existing file, if available.\n",
    "    if os.path.exists(path):\n",
    "        glove = np.load(path).item()\n",
    "    else:\n",
    "        f = open(gloveFile,'r')\n",
    "        glove = {}\n",
    "        count = 0\n",
    "        for line in f:\n",
    "            split_line = line.split()\n",
    "            word = split_line[0]\n",
    "            embedding = [float(val) for val in split_line[1:]]\n",
    "            embedding = np.asanyarray(embedding)\n",
    "            glove[word] = embedding\n",
    "            count += 1\n",
    "            if count >= word_count:\n",
    "                break\n",
    "        # Saves the vectors, so we can load it faster next time.\n",
    "        np.save(path, glove)\n",
    "\n",
    "    return glove\n",
    "\n",
    "def load_data(loc='./data/maluuba/data_frames.json'):\n",
    "    \"\"\"Loads data directly from JSON. Input data should have the form:\n",
    "        ...what about stop words???\n",
    "    \n",
    "    :param loc: string\n",
    "        Locates the directory in which the data is saved.\n",
    "\n",
    "    :return: X_list, list\n",
    "        List of all the dialog turns as a list of strings. \n",
    "        Example:\n",
    "        ['First conversation first turn.','First conversation second turn.',\n",
    "         ..., 'Last conversation, last turn']\n",
    "    :return: Y_list, list\n",
    "        All labels corresponding to each dialog turn as embedded lists.\n",
    "        Example:\n",
    "        [['label_7'],['label_2'], ['label_4','label_1'], ['label_3'], ...]\n",
    "    \"\"\"\n",
    "    X_list, Y_list = [], []\n",
    "    \n",
    "    with open(loc, 'r') as file: # Add maluuba to input\n",
    "        for line in file:\n",
    "            line = json.loads(line)\n",
    "            dialog_list = line.get('dialog_list', 'empty')\n",
    "            labels = line.get('labels', 'empty')\n",
    "            \n",
    "            # Since, for now, each dialog is an example i.e. we ignore context\n",
    "            X_list.extend(dialog_list)\n",
    "            Y_list.extend(labels)\n",
    "    return X_list, Y_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences2vec(sentences, glove):\n",
    "    \"\"\" Uses the glove word vectors to convert whole sentences to single\n",
    "    vectors by averaging the input vectors.\n",
    "    \n",
    "    :param: sentences, list\n",
    "        List of all the dialog turns as a list of strings. \n",
    "        Example:\n",
    "        ['First conversation first turn.','First conversation second turn.',\n",
    "         ..., 'Last conversation, last turn']\n",
    "    :param glove: dictionary\n",
    "        {'word': vector}, where vector has shape, (1, 'n_features')\n",
    "\n",
    "    :return: list\n",
    "        List of sentences, where each sentence is represented by a\n",
    "        single vector.\n",
    "    \"\"\"\n",
    "    if type(sentences) != list: \n",
    "        print \"sentences2vec requires a list of sentence/s\"\n",
    "        return\n",
    "    \n",
    "    sentence_vectors = []\n",
    "    for sentence in sentences:\n",
    "        sentence = word_tokenize(sentence)\n",
    "        matched_words = len(sentence)\n",
    "        \n",
    "        # All vectors have the same dimensionality. Using an example\n",
    "        # 'word' to set the size of our new sentence vector.\n",
    "        sum_of_words = np.zeros(len(glove[',']))\n",
    "        \n",
    "        # We will represent a sentence as an average of the words it contains.\n",
    "        for word in sentence:\n",
    "            sum_of_words += glove.get(word, 0)\n",
    "\n",
    "        if matched_words != 0: # Necessary, to ensure we don't divide by zero.\n",
    "            sentence_vector = sum_of_words/matched_words\n",
    "            sentence_vectors.append(sentence_vector)\n",
    "\n",
    "    return sentence_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def full_trn_tst(data_in, test_size, num_labelled, args):\n",
    "    # Dictionary {word:vector}, where each word is a key, and the value is a\n",
    "    # row vector of shape (n_features,).\n",
    "\n",
    "    model = load_glove_model('./data/glove.840B.300d.txt')\n",
    "    X, Y = load_data()\n",
    "    print 'Sentences and labels loaded.'\n",
    "\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    \n",
    "    # Y is a numpy array, shape (n_samples, n_classes)\n",
    "    Y = mlb.fit_transform(Y)\n",
    "    \n",
    "    # Place each example (turn in conversation) as a vectors, as a row in a \n",
    "    # matrix.  X has shape (n_examples, n_features)\n",
    "    X = np.asarray(sentences2vec(X, model))\n",
    "\n",
    "    \n",
    "    # Preprocessing:\n",
    "    # X = StandardScaler().fit_transform(X)\n",
    "    # if args.pca: X = process_pca(args.pca, X)\n",
    "    # else: pass\n",
    "\n",
    "    # Randomly split the X and y arrays according to test_size.\n",
    "    X_trn, X_tst, Y_trn, Y_tst = train_test_split(X, Y, test_size=test_size)\n",
    "\n",
    "    # We are giving the Dataset 'num_labelled' fully labelled examples \n",
    "    Y_trn_mix = Y_trn[:num_labelled].tolist()+[None]*(len(Y_trn)-num_labelled)\n",
    "    \n",
    "    # Dataset is a libact object that stores labelled and unlabelled data \n",
    "    # (unlabelled examples are stored as None)\n",
    "    trn_ds = Dataset(X_trn, Y_trn_mix)\n",
    "    tst_ds = Dataset(X_tst, Y_tst.tolist())\n",
    "    fully_labeled_trn_ds = Dataset(X_trn, Y_trn)\n",
    "\n",
    "    return trn_ds, tst_ds, fully_labeled_trn_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1367 conversations, gives us 19984 examples\n",
    "# Note there are 20 classes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py2]",
   "language": "python",
   "name": "conda-env-py2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
